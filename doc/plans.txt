cmpr design ideas / plans

Files and blocks
----

20240409 Wed

Files can be empty, but blocks are never empty, which means an empty file contains no blocks.

We maintain the clean definition of blocks (they are non-empty spans of texts) and we also respect the idea of files.

Therefore, j/k order is a union of the file list and the block list.

We do away with the earlier idea of empty blocks only in empty files.

As an example, if the project contains files a, b, c, and d containing the following blocks:

a: 1 2 3
b: no blocks
c: 4 5
d: 6

In j/k order (formally, just kidding order) we visit three blocks for file a, and the empty file b, and then the fourth block in file c.

Unlike as of v5, we do not count a block index for the empty file b, which means you can add an empty file to your project to record the intention to store some blocks there in the future without changing the number of blocks in your project.
The language of empty files does not even need to be defined, at least not for ingest.
Files and blocks are separate lists, but they have a total order.

Now we also say (as of v6, say) that every block has a checksum.
The checksum isn't guaranteed to stay the same between versions, but the tool should be responsible for updating it, which means at least one overlap between checksum implementations to support version upgrading.
If the file is very short we may just store the content itself.

For now, we are assuming that the only use we have for the checksums is that we can identify whether a file contents have changed.
If they have, we can identify changes to individual blocks if the file is divided into blocks.
However, as the checksum function we are using is of course opaque, we will not be able to measure block similarity, only content-identity, and that only probabilistically.

The concept of empty files also makes vi-like normal mode commands like d, p, o, useful to do things like take a block from an existing file with d and move it into a new file with p, even if that file was empty.
Obviously this will get more useful when we have actual content-aware hashing.








Basic operations
----
20240310

The basic operations are all on blocks.

They are:

r/R (currently split into two parts but logically one operation)
d (not implemented)
o (not implemented)
e
x (not implemented)
p/P (not implemented)

These are all modifying operations, but we also have:

undo
redo
diff

This family of operations change the block contents, but not necessarily to something new.
Undo is "u" in vim, but redo requires a modifier key.
I'm not sure if we want to keep these exact keyboard shortcuts, especially as modifier keys are difficult in some environments (e.g. browsers).

We also need new keys for the inverse operations of "r/R".
This really needs, to be full-featured, to include getting the full block into the clipboard, getting the comment part or code part, and optionally applying some transformation to either one of these.

An easy DSL to express this is the following:

prompt("writethecode",commentpart(block))

In this idea, block is a local variable, commentpart and prompt are functions, and the string literal "writethecode" is an identifier for a prompt template which in this case takes a single argument.

Arguments to prompts are always positional and can always be empty.
The prompt "writethecode" might be given on the conf file or could come from a block of prompt templates, or another conf file, e.g. a "writethecode" file in a .cmpr/prompts/ directory or similar.









Implementation plan
----
20240310

When we read in data from disk (in get_code) we should also do all indexing operations.

This means we get checksums for all our blocks and all lines.

In particular:

- we make a lines spans which we store on ui_state
- this will be kept up to date for data that is in memory only

It is intended that the backing file always contains the same lines as we have in inp.

However, as we do not exclusively own the file, we always check it when opening a project and again before writing.

Specifically, when we write, we first copy the file, if it exists, to a .bak copy.
We read the bytes of this file and compare them with what we have in memory.
Note that in new_rev we still have the previous contents of the file (since that is where we update file.contents and inp and therefore the block contents as well).




























Networking Plan
----

A revstore:

- stores blocks
- serves lists of block hashes which it has
- serves block contents for a given hash when asked
- only allows known users to write to it
- may have access controls on who can read from it
- (necessarily) has rate limits on all access for all users

Because a revstore is controlled by a certain author, it's a great place to get vetted news, if you trust that author (or curator).

Guiding principle:

Whenever possible, determine everything from the revstore contents itself.

> cmpr --rvs-server

Or perhaps just

> cmpr --server

























Outputs
----

Let's say the filetype is HTML.
It would be natural to be able to view the output (maybe continuously updated) without having to use the (B)uild command just for that.
So let's say we have "V" or :view for view, and this could include a 20px fixed header with a history bar.
The current contents will automatically refresh.

However, we might not want to treat an HTML file (particularly a generated one) as a source of blocks.
This means if we have Library: we should have View: as well.
The point is to bring the view into the workflow (for example, as an input to some procedure) but not add blocks.
Note that you can still edit a file.
The necessity for View: to open an external browser is a limitation of the terminal interface.
In particular, it will not play nicely with SSH.
A :view-url would be a nice way to get a network-friendly https url to fetch HTML from.

































Block chains
----

As a simple example of a more general idea, we propose chains of blocks as an include mechanism.

As an LLM requires input to be provided in some order, each block can strictly speaking only have one block immediately preceding it.

The idea of a block chain is just this: each block may name its immediate predecessor.

The syntax for this can be as follows:

<block-start-token> #name-of-block @predecessor-name

This suggests two basic features:

1. An index of block names to block indices.

2. A transform on blocks that expands references (transitively) and removes references if desired.

The implementation is like this:

expand_blockrefs: Block -> State BlockChain
expand_blockrefs Complete b := b
expand_blockrefs Child b := expand_blockrefs (expand_parent b)

Where expand_parent simply concatenates the parent block to the block itself.
Obviously, expand_blockrefs can be extended to have multiway branching, to intelligently handle cycles, and so on.

This can be seen as a specific case of a more general pattern:

1. look up a block
2. do something with it

In the case of r/R this is split the comment part, wrap it with a prompt template, send it to an LLM, and replace the remaining suffix of the block with the LLM output.
There's a human in the loop, which is enhanced if we have both "r" (regenerate) and "u" (undo) in the same UI.

The idea for "r" implementation is that if you turn the API on, it just does it.
Then you have "u" to go back, and perhaps some kind of diff view.
I think "U" could be for the diff view features and an undo/redo list; like search mode it would be a new view.
It might show a scrollable list of all revs of the current block.
It would show commands on the ruler, such as space to select/mark a rev, d to see diffs between marked and current, perhaps m to do a three-way merge (you select three revs, it puts all the lines that were in any of them into the same file and you edit it), and so on.

An idea nearby the block parent sigil is one for block filters, each of which is just a function applied in order.
For example:

<block_start> #some_block .func1 .func2

Would produce func2(func1(block contents)).

























Name spaces
----

A name space is a list of pairs of checksums and names.
It is published by a revstore which also hosts blocks having all the given checksums.

There can only be one name for a block in a given namespace.

As an example, I can publish at spanio.cmpr.ai my own namespace for the spanio library.

I can also publish all of the blocks.

Then you can run any of my code by getting an index block from my server, listing the name space, searching the names, and accessing any of interest.
































Third half
----

The first half of cmpr was going from NL to PL (e.g. English to Python).
Second half is going the other way, PL to NL, which is critical for working with existing codebases.

The "third half" is the diff layer.

If you have a block and you want a better version of that block, you want to be able to describe the difference in English and have ChatGPT make it happen.

Between any two blocks a, b, there is a diff d(a,b) that represents the change from a to b, a delta or derivative.

A very natural operation is describe_diff(a, b) which is like diff(a, b) but it adds an English explanation.

This can be something like "This fixes a bug in the code by changing a pointer from an incorrect to a correct value" but a more useful description would include variable names and similar detail such that the patch can likely be recreated.

In common use with ChatGPT, the user may say "the code needs to be fixed to do ..." and the LLM replies with a suggested change.
This is going from an English diff to a diff in PL code.

The reverse operation is also interesting: given a change in PL code, find the corresponding change to the English (NL) code.

In fact, as there are both NL and PL texts and also NL and PL deltas, there are at least four interesting arrows: NL to PL is handled by the LLM (mainly), PL to NL is sometimes done when debugging or when ingesting code.
A NL delta may be taken as a change to NL or to PL, similarly a PL delta may be a PL change (e.g. a standard diff) or may be interpreted (by either human or LLM) as a change to NL.

We can also define different kinds of deltas.
For example, a unified diff is a textually unambiguous delta, but a description in English of a change is also a kind of delta.
The LLM or the human may convert between these.
In particular, conversion between English descriptions of changes to the code and unambiguous diffs to the original English description is how we go from an English conversation (ending in a complete program) to a single English program (resulting in a functionally identical PL program).

For any block we can define at least three significant kinds of neighbors.
First we have the parent block in time.
This is whatever it was before you edited it into its current state.

Second we have some block chain parent.
This is the pointer to the foundational information on which we depend.
(The root node by the way could be considered implicitly to point to GPT4 as currently we don't expect it to work on other models without testing.
This is a way of saying we depend on some knowledge built into the model.)
This is a temporal relationship loosely speaking, but can be edited easily, e.g. when a bootstrap block gets split into subcomponents.

Third is nearby blocks in some metric space, such as nearby programs.
An example would be our functions that split files into blocks, which are very similar to each other but with minor differences for each PL.

























Implementation plans
----
20240310

First we set up the indices: lines, blocks, files.

Then we review this file and work out a plan.

But before any of that, it turns out, we are actually implementing openai-key support.

We get the openai-key from a file .cmpr/openai-key which must be owned and readable only by the current user (i.e. chmod 0400).

The end library experience we want should be span result = llm(span).

We want to record all our API inputs and outputs.

Before calling llm() we will call some other setup functions.

So the API we want from the rest of the code is just this:

- setup_llm(): does whatever is required to be able to use the LLM
- llm(): uses the LLM
- free_llm(): does whatever cleanup is required

All the specifics of what the LLM is and what these steps are can change by configuration.

(Basically implemented this, but with the model name instead of llm() as the function name.
 I don't think supporting just one LLM at a time is good enough since you'll likely want different ones for different blocks or different commands.
 The argument to llm(), though not settled here, turned out to be a json array of messages, each with role and content, as in the OpenAI chat API.)








Planning for block reference features
----
20240501

Currently the bootstrap prompt is special: it's generated by a script, which can pull in other blocks.
We can recreate the functionality with some more natural features:

- you have a shell block that generates output directly, e.g. `find . -type f -name '*.py'` lists files in your project.
- the output of this block can be included (indicates the idea of a block's output as an addressable blob).
- a single block can be distinguished as the bootstrap block and it will call in everything else needed.








Planning for delete
----
20240501

The plan is that when a block is deleted it will be added to a file (e.g. .cmpr/deleted) and then when it is pasted somewhere else it can come from this file.
That means we don't have to use revs for this (although we could) and we also don't have to keep deleted blocks in memory.
It also means that deleted blocks automatically persist across restarts.
A side-effect could be that, since this deleted file would be appended to rather than replaced when deleting a new block (perhaps...) then it would actually give us a stack.
So if you delete two blocks and then put two blocks, you would delete A B and then put B A.
So ddPP would reverse two blocks.
Also dp would reverse two blocks, just as ddp does in vim.
The difference is that we are assuming 'd' deletes a block, rather than 'dd' in vim for a line, since I'm assuming 'v' or other modes will be used to select multiple blocks (or a block selection mode, perhaps 's' to toggle).

However, there's a huge problem with appending multiple deleted blocks to the same 'deleted' file, namely that blocks deleted from multiple filetypes can't be stored in the same file at all.

So, instead, since we are already doing content hashing of blocks, we could have instead a deleted block directory (.cmpr/rm/ or so) and in that directory have deleted block contents, each in a file named by the blocks content hash.

Then in order to undo we need to know somehow that the block was deleted.

Or: what if we have simply a stack of deleted blocks, by content hash, and "p" by default puts the last deleted (or yanked, but we'll get to that) block into the current position, but "P" can let the user choose from all deleted blocks, most recent first.

So with undo, we have "u" to just undo the most recent change (to the current block, say, or perhaps to any block project wide as in vim), but "U" will open up a UI to choose from any number of undo dimensions (last change across project, last change or any previous version of current block, last change to current file, etc).
Then with "d" we would simply delete the current block (or selected set) while "D" would naturally give us some kind of options.
This might be: delete the file, delete the block, or maybe view deleted blocks(???)

However there are some problems with this, namely p/P is hardwired brainprint for vim folk.
I think making d immediate is fine, as it will quickly be learned, but changing something like P would just be bad.
So, perhaps a better idea is to revisit the theory of vim keybindings and come up with something similar but different.

First of all we have motion commands combined with operations, like d2j.
I think a better idea for cmpr is some basic operations like d, p, u, o, e, x, r, i, hjkl, ., and some modifiers, probably coming before them, that lead to modified operations, but not necessarily always by a motion.
For example, using ? as a placeholder for some as-yet undetermined modifier, we might have ?h giving options for moving, and ?i giving options for going into edit mode.
In fact, ? is perhaps good for this, or perhaps terrible as it's bad to type.
Many vim keybindings are specific to sub-block motion.
We have repurposed line motion to block motion, and all word or character motion does not have a direct analogue.
We can repurpose at least: a/A, b/B, c/C, e/E, f/F, H/L/M, i/I, J, K, m, q, s/S, t/T, w/W, x/X, z.
We already have plans or uses for: d, g/G, n/N, o/O, p/P, r/R, u, v, y.

Current plan: use ? as a prefix to go into an expanded "panel/help" mode with more options for a command

TODO:

bootstrap happens on startup and it tries to copy to the clipboard, which means cbcopy gets prompted








Refactoring with block references
----
20240510

Now that we have block references, it's time to split things into smaller parts and see what happens.

Here I want to have some metrics on before and after code size.

The total number of blocks currently is 172.

This can change when files change type, as some files are still "all one block" style.
So a better metric is code blocks, which we can approximate as blocks in the cmpr.c and spanio.c files.

15-116  cmpr.c
117-138 spanio.c

In spanio, since it is hand-written, there are many fewer blocks.

We can get some metrics at the command line for PL and NL lines and bytes.

Application code: 
4311 lines, 1991 NL, 2422 PL
169kB,      96kB NL, 72kB PL

Library code:
1602 lines, 417 NL, 1207 PL
52kB,      22kB NL, 30kB PL

~ $ (cd cmpr; for f in $(seq 15 116); do dist/cmpr --print-block $f; done) | wc
   4311   24640  169285
~ $ (cd cmpr; for f in $(seq 15 116); do dist/cmpr --print-comment $f; done) | wc
   1991   16903   96901
~ $ (cd cmpr; for f in $(seq 15 116); do dist/cmpr --print-code $f; done) | wc
   2422    7737   72486
~ $ (cd cmpr; for f in $(seq 117 138); do dist/cmpr --print-block $f; done) | wc
   1602    7882   52299
~ $ (cd cmpr; for f in $(seq 117 138); do dist/cmpr --print-comment $f; done) | wc
    417    3814   22369
~ $ (cd cmpr; for f in $(seq 117 138); do dist/cmpr --print-code $f; done) | wc
   1207    4068   29952

Observations:

There's more lines of PL code in cmpr than lines of NL code, but when measured in bytes, there's more NL code.
This makes sense as our lines of English prose are on average much longer (usually whole sentences).

In the library, as it's all hand-written, this doesn't tell us much.
There's a lot of commented-out code and a lot of documentation but also design rationale and so on.

As we refactor it would be interesting to see these change, so we can script this.
However, the number of blocks is going to change as we move things around, so nevermind.

We are going to probably mix some refactoring and dead code elimination, but maybe we can eliminate dead code first.
Then we can get a better sense of the change in NL code size due to block references.
I'm aware mostly of pagination-related dead code, so I'll start there.

Update: 20240513

I actually did very little refactoring as I decided instead to focus on building experience with block references by adding new features.
However, let's update the numbers above to see what I did do:

14-112  cmpr.c 
113-139 spanio.c

Before:
Application code: 
4311 lines, 1991 NL, 2422 PL
169kB,      96kB NL, 72kB PL

Library code:
1602 lines, 417 NL, 1207 PL
52kB,      22kB NL, 30kB PL

After:
Application code: 
4265 lines, 1980 NL, 2384 PL
167kB,      96kB NL, 71kB PL

Library code:
1700 lines, 504 NL, 1223 PL
57kB,      27kB NL, 30kB PL

~ $ (cd cmpr; for f in $(seq 14 112); do dist/cmpr --print-block $f; done) | wc
   4265   24287  167659
~ $ (cd cmpr; for f in $(seq 14 112); do dist/cmpr --print-comment $f; done) | wc
   1980   16740   96070
~ $ (cd cmpr; for f in $(seq 14 112); do dist/cmpr --print-code $f; done) | wc
   2384    7547   71688
~ $ (cd cmpr; for f in $(seq 113 139); do dist/cmpr --print-block $f; done) | wc
   1700    8755   57640
~ $ (cd cmpr; for f in $(seq 113 139); do dist/cmpr --print-comment $f; done) | wc
    504    4646   27313
~ $ (cd cmpr; for f in $(seq 113 139); do dist/cmpr --print-code $f; done) | wc
   1223    4109   30354

There's little change; I did clean up some pagination-related dead code.








Adding indexing and an Undo feature
----
20240513

We'll index the blocks in the current code first.
Then we'll index the blocks in the revs.

We're going to checksum every file, every block, and every line.

Since this indexing will be shared between current files and revs, we'll plan for that.

Currently we use `find_all_blocks` in `get_code` and we call `checksum_blocks` but it is currently empty.

We can store the checksums of the blocks on the state, in an array.

We can also store the checksums of all the lines.

Now we need an algorithm to identify previous versions of blocks.

Also, we might as well index the block ids now, which `get_code` should also do.

So for each block we get an id out, if any, and we get a list of 64-bit line hashes.

Now when we are doing block ID lookups, we will do it right, instead of using `find_block` which is for full-text search.

We can have on state a `block_ids` spans, having the same length as blocks itself.

Then when we need a block by ID we will just do a linear scan of `block_ids` and map that index back onto the block itself.

If we find duplicate block IDs after indexing, we can report that to the user and then continue.

We should regard the extraction of the block index itself as a function span -> span, and the indexing operation just treats this as one particular special case.
In other words, we could easily add other indexing functions in the future for other purposes.

(In fact, blocks themselves are simply an index that's treated specially in the UI.)

(We could treat ctags and similar as an index as well, and allow referencing arbitrary functions to pull their code into blocks as well.)

We also need to index when code changes, but we're doing `find_all_blocks` I think when code is being updated, so we need to do all the things in all the relevant places.

It makes sense to me if `find_all_blocks`, since it is scanning anyway, also does the full block checksum, line checksums, and block ID indexing.

Or perhaps for clarity these should be separated.

Actually, `find_all_blocks` is already doing a lot, so let's separate them.

After we find all the blocks, we will index them.
So in `get_code` we will call a new function `index_all_blocks`, which will allocate a spans and call a new `get_block_id` function, a checksum function, and a checksum lines function, for each block.

The block checksums can also go on the state in another array of u64, maybe aliased as a checksum type.

If we do these for each line, they will be big.
Well, in this codebase we have 24k lines, so still trivial, but ~3-4 orders of magnitude bigger when we count all the revs.

There is an interesting way to find similar blocks or spans using checksums of lines:

- Choose at random some set of bits (in 0-63) to use of size k (= a smaller hash).
- Set up k counters (possibly log-stochastic) of a reasonable size (easy here since blocks are small).
- For each block, for each line, increment each counter when the corresponding bit is set.
- Then to compare one block with another block, subtract the counts.

What we actually want is a Jaccard index, which is the ratio of the intersection and union of two sets.

The problem with estimating this by counting bits is that each bit will be the same 50% of the time by random chance when the lines are unrelated, which makes the counts less valuable, as each one will tend towards 50% of the counted lines.

We can solve this problem by picking some random pattern (for example 0101 or 0000 or 1111) of m bits.
Now we count the occurrence of these patterns at k positions, instead of counting k bits.
Now the probability of two different lines matching by chance is $2^m$.
The counts will be lower, and the variance will be higher.

To estimate the similarity from the counts, we take for each of the k counts, the min count for blocks A and B.
These represent probabilities of overlap between the two blocks.

If these are 0, it means the blocks are disjoint.
If the blocks are disjoint, however, it might be above 0.

We can see about that there are on average about 44 lines per block in our application code.

We might assume that between revisions there are up to 50% of lines changed.

So our problem can be stated as, for a block B of around 40 lines, and some collection of other blocks of similar size, find the ones that have 20 or lines in common, with some high probability of success of finding each one.

Here's an idea:

For the first k bits of the hash, for n lines in a set (a block) we find the most common pattern (the mode of the distribution) in the set.
For example, if k is 1, we find out whether more lines have a 1 or a 0 for the first bit of the hash.
This will be the same by chance 50% of the time for two random blocks.
However, for k=2, we are picking 1/4 of the lines at random, four different ways.
If there are repeated lines, this will clearly identify them, but let's assume that all the lines are distinct.
(We can of course make them distinct by deduplicating the lines as a first step; in particular this removes blank lines which might otherwise dominate.)

If the lines are distinct we are randomly partitioning them into 4 subsets.
Essentially we are looking for "surprising facts" about the set.

Specifically we are looking for surprising facts that are true of a surprising number of the members of the set.
This is why we want to look for high numbers, not low numbers, in the counts of the pattern.

Let's say we have 32 distinct items.
If we look at k = 2 bits there are four buckets, but in a 64-bit hash there are 63 contiguous 2-bit ranges that we can easily look at.

Let's say we do this and we find the most surprising of these facts.

In fact, let's just look at single bits.

The chance that there one particular single bit being set for all 32 distinct 64-bit hashes is approximately $2^{6}/2^{32}$.
(This is 64 times the probability of a single bit being set by chance 32 times.)
(This is actually the EV of the number of such bits, but close enough as it's closer to 0 than 1.)

Let's assume we can gain the most information the cheapest by looking for things that should happen around 1/32 times by chance in a set of 32 distinct lines, but that happen as many times as we care to look for.
For example, for 1/32 we have 5 bits, and there are 64 choose 5 = 7624512 sets of bits that we could examine (just considering bits that are set, not arbitrary patterns).
There are also heuristics we could use (like looking at single bits first, and then trying the surprising ones in combination) to find surprising facts more efficiently.

That is, we could pick 5-bit patterns, (for example, just picking five bits, with the assumption being that each of those bits must be set; then our representation of the pattern that we have found is just the set of those bits, i.e. 5 numbers in [0,63]) and examine them until we find something that is sufficiently unlikely but true about our set.
Then we can be as confident as we like, when we find another set for which this is true, about the similarity that we have found.

This seems a bit smarter than minhash to me, as minhash is finding something unlikely about a set member, but we are finding something unlikely about the set, but I don't really understand minhash.

This means, as $64 \choose 5$ is within an order of magnitude of $2^{26}$, we have a chance of finding something about as unlikely as one bit being set on every hash.
How many lines will this unlikely thing entail?

Apparently, around 11 lines.

```
import scipy.stats as stats

 # Parameters
p = 1/32
n = 32  # Adjusting n accordingly to match the lambda approximation

 # Poisson parameter
lambda_ = n * p

 # Find k such that P(X >= k) is approximately 10^-9
k = 0
cumulative_prob = 0

while True:
    cumulative_prob = stats.poisson.cdf(k, lambda_)
    if 1 - cumulative_prob <= 10**-9:
        break
    k += 1

k
```

This seems like it should objectively work pretty well.

As per ChatGPT:

To estimate the time required to evaluate 32 64-bit hashes, masking all possible patterns with five bits set (64-choose-5), we can break down the task into several steps and consider the computational cost for each step. Here's the process:

1. **Calculate the number of 64-choose-5 patterns:**
   \[
   \binom{64}{5} = \frac{64!}{5!(64-5)!}
   \]
   This number represents all possible ways to choose 5 bits out of 64.

2. **Evaluate the computational steps:**
   - For each of the 64-choose-5 patterns, you need to:
     - Apply the mask to each of the 32 hashes.
     - Check how many of these masked values have all five bits set.
     - Track the mask with the highest count of hashes having all five bits set.

3. **Estimate the number of operations:**
   - Number of masks (combinations): \( \binom{64}{5} \)
   - Number of operations per mask:
     - 32 hash evaluations (AND operation with the mask).
     - 32 bit-checks (checking if the 5 bits are set in each masked hash).

4. **Total number of operations:**
   \[
   \text{Total operations} = \binom{64}{5} \times (32 \times \text{AND} + 32 \times \text{bit-check})
   \]
   Assuming AND and bit-check operations are single CPU instructions.

5. **Performance on a modern CPU:**
   - Recent Threadripper CPUs (e.g., AMD Ryzen Threadripper 3990X) have high core counts and can execute many instructions per cycle.
   - Let's assume a conservative estimate of 4 GHz clock speed and 4 instructions per cycle.

 ### Step-by-step estimation:

1. **Calculate 64-choose-5:**
   \[
   \binom{64}{5} = \frac{64 \times 63 \times 62 \times 61 \times 60}{5 \times 4 \times 3 \times 2 \times 1} =  75287520
   \]

2. **Total operations:**
   \[
   \text{Total operations} = 75287520 \times (32 \times 2) = 75287520 \times 64 = 4818401280
   \]

3. **Instructions per second on Threadripper:**
   - Clock speed: 4 GHz = 4 \times 10^9 cycles/second
   - Instructions per cycle: 4
   - Total instructions per second: \( 4 \times 4 \times 10^9 = 16 \times 10^9 \) instructions/second

4. **Time to complete the task:**
   \[
   \text{Time (seconds)} = \frac{\text{Total operations}}{\text{Instructions per second}} = \frac{4818401280}{16 \times 10^9} \approx 0.301 \text{ seconds}
   \]

This rough estimate suggests that the task would take approximately 0.301 seconds on a modern Threadripper CPU. This assumes that the operations can be perfectly parallelized and that there are no significant overheads or bottlenecks. In practice, actual performance may vary due to factors like memory access times and CPU architecture specifics, but this provides a general idea of the computational effort involved.


One or two questionable remarks there but seems like a pretty solid analysis overall.


So here is the basic idea:

- we find a fingerprint for a target block, or for each block; this operation can be costly
- we search given a fingerprint over all other blocks; this operation needs to be cheap

In particular, we are searching backwards chronologically looking for the first block that is the most similar to the current block.

So here's what we do:

We have a function that takes an array of n-bit hashes and an integer k.
It tries all nCk masks and returns the one which matches the highest number of the hashes.
(We could double the performance by using popcnt and also finding the ones where none of the bits are set.)
(The mask is a number, this reminds me of Rabin fingerprints. Maybe it's exactly the same idea?)
Then we return the mask and the count.

We run this function on all blocks and store the results.

To find the previous version of a block, we scan the blocks most recent first.

For each block, we compare the number of lines that match that mask in the hash.
(We should also have the hashes stored for each line, of course.)

If it's 11, we probably have the same block, but even if it's 5, very likely the same with some changed lines.

The chance of 5 by random chance (with 32 lines, none actually matching) is $\approx 32 \times 1 / (2^{5})^{5}$.

So I'm going to implement this idea without any further analysis, and then focus on empirical evaluation of the results before considering alternative approaches.




HOWEVER:
----

Perhaps there's an even better idea.

After some napkin math, and the realization that at the moment, I only care about 1-by-n comparisons, i.e. find the last version of this particular block (for undo) and not n-by-n searches for all similar pairs of blocks at once, I can do something much simpler.

Namely:

For each block, we have a list of 64-bit integers, one per line.
We sort these 64-bit integers, for each block, once when we index them, and keep the sorted list (or both as a permutation).

To compare a block with a previous block, going back through the history of blocks, we simply count the number of shared lines they have in common.
This is a linear scan of ~ 40 pairs of sorted 64-bit integers, assuming ~40 lines in the average block, so about 1600 operations.

This means if we have ~100 blocks in the project and ~1000x more in the history, then even allowing for some growth in each of those numbers, we have maybe 10 million blocks total in a moderately large project.
At least until we are importing linux kernel-sized projects, a linear scan should be fine for an undo feature.
In most cases, we will only need the first result or the first few anyway, and this should be easy to find quickly.

Another point, somewhat unrelated, is that we need to know the type of a file before we can even find the blocks in it.
For revs, this will have to be heuristic since we've decided not to store that information.
So there are ways to approach this, but we also want to know what the file is, perhaps... or maybe we don't even care, just about block identity.

OK, assuming we don't care, we can identify them by a heuristic over lines that are in blocks that are in that language...

In other words, we could have another index of lines by language.
Then when we look at a file, we could look at that to identify the language to use to blockize the rev.

Or, we could even run all the blockizing routines over each file, and then see which set of blocks makes the most sense...

But there is one idea that I like in all this, which is that letting the user choose the filetype it today's conf file, and then applying that retroactively to that file's revs seems nice.

For example, if I add Markdown support, and then change a file's type to that, even though it was formerly treated as C or plain text, it makes sense to blockize the older revs as Markdown and not suddenly switch to a single-block file at some point in the history.

This suggests that we take the current files, look at each rev, and simply compare the lines in order to decide which file that is an earlier version of.
Once we make that decision, we can go backwards with it, i.e. we can treat that version n-1 as the version that we are matching against as we go even further back, and in this way we get a chain of revs for each current file.

So that's what we do, and there's now a complete implementation plan.








Implementation plan
----
20240513

For the revs and diffs and undo features.

First, we read all the files in the current project.
For each one, we hash the whole file, each line, and each block.

The lines we will store in a sorted list for the entire file.
We will do the same for the lines in each block.
We will also have a linear list of the line hashes for each file (and block).

This probably means simply that we should have a `lines` spans, like we do with blocks.
We can get the block for a line in the same way that we now get the file for a block.

A side point: separating out concepts like "here's how we handle location-based reasoning on spans" to a single block (for referencing) makes the program dramatically better for a human reader as well (previously this was implicit in the set of primitives exposed by spanio and how they were used).

We then load the revs.
For each rev we load the file, read and checksum the lines for the file, and then compare against our working set of file-versions.
Initially this working set is of course just the current projfiles.
However, when we find a new version (that is, a previous version) of a file in this set, then this replaces that file in the set.

Also, if we need to for performance reasons, we can do this only for one rev of each file, i.e. we can look at the last rev we have for each file, and then stop, temporarily, going back for that file, at least until we need to for some user action.

(We could also do all this on demand instead of on load.)

For now we assume it is fine to do the entire history in one go.

Once we have a nearest-neighbor file for a rev, we take the language that file is currently set at (from our working set) and blockify the file based on that language setting.
Then we add the blocks to another structure, which is "blocks per rev".
We checksum each block, and associate each with a (sorted and non-sorted) list of line hashes.

When the user asks to see the undo list with "U", we then go back, not just to the previous versions of this file, because that block could have been moved around, but rather to a chronological list of all blocks.
(The order of blocks from the same rev is undefined in this list, so it's a partial order.)
The first block that we find that is similar enough we display.

When we are searching through the blocks, we should, after every few loops, clear the screen, and print the progress of blocks we have searched and blocks to search through.
Once we have searched all of them, we will show the most recent matches first, and allow some kind of moving around between them with j/k.

To this screen we can eventually add other features like displaying a diff, hitting "space" to mark a particular version as the one to compare against, and so on.
But for now, we'll just show the list.
If you hit Enter on a different version, we'll just make that the current one and leave the "U" view.

So now we think about what data structures we need.

- line hashes by rev, sorted (for file identification)
- line hashes by block, sorted
- the "working set" of file revs as we go backward
  - one language per file/rev
  - the current filename, probably, if we even care(?) (probably just the index)
  - the rev, of course (the filename)
  - the timestamp, probably converted to Unix time
- the chronological list of blocks
  - the sorted line hashes
  - the timestamp
  - the id if there was one

We should make these structures dynamic, in that they are intended to be extended (backwards through time) as they are used.
In particular, we probably will not want to populate all the block histories on startup.
However, for now, we should treat these as later performance enhancements, and start by brute-forcing everything.

We might want to have a "debug flags" conf variable.
Adding letters to this, which will be by default empty, will add debugging information in the UI, such as a cmp space high-water mark in the ruler.
Or instead of letters, maybe a comma-separated list of items like "cmp" for the aforementioned.

So now in terms of blocks and implementation:

Currently we load all the code in `get_code` and then we call `get_revs`.
This suggests we can work on everything in `get_code` first, and then revisit the implementation plan for `get_revs`.

For storing the line hashes, we will need a new structure.

We might as well have a spans for the lines, which means 64 bits * 2 per line.
This is of course wasteful, but we don't care yet.
This is 16 bytes of waste per line, and as we have seen we have about 40 bytes per line on average anyway.

We can waste the same amount again for the sorted sets per file.
We can calculate these for all revs in `get_revs`.

In `get_code`, once we have loaded the files and the blocks, we will now do all the checksumming, letting us work out the structures for the line hashes and sorted line hashes.
For example, the sorted line hash can be another copy of the same hashes in a permutation, or we could simply store the permutation (for example, as a sequence of jumps).


Note:
As seen in the following example:

`/* #print_block #print_comment #print_code #count_blocks`

Sometimes we might want a single block to have multiple ids (when it implements multiple functions).
Should we support this? Maybe?









Design notes on `top_refs`
----
20240515

Problem statement: we have a block and we want to expand the top refs and the inline refs such that the top refs can be put into a separate LLM message.

Both the top refs and the inline refs need to be expanded recursively.

However, at the 0-level of recursion (in the block itself) we only want to expand the top refs, so that these can be returned separately.

This suggests that we have a function Block -> top expansion, and Block -> body expansion, and Block -> full expansion.

The top-level routine that prepares the LLM call will need to use the first two, to prepare the separate messages.
The recursive component will always use the full expansion.

When doing body expansion, we want to strip block comment delimiters, so that we aren't putting a block comment inside a block comment.

This means that there is a context within which the recursion needs to operate differently.

So one approach is to parameterize the function, so that it either expands the top, expands the body, expands the entire block, and also so that it either strips or doesn't strip the comment delimiters.

The comment stripping parameter will persist through the downward recursion, but the expansion will always be determined by the transform if any on the reference itself.
By default it will be a "comment" transform, but we will strip the comment delimiters if we are already in a comment context.

This means that if we are doing a full expansion in a comment context, we need to recursively expand the comment part with delimiter stripping, then the code part.








Idea
----
20240516

I'm noticing that my `#all_functions` block is kind of unique in an avoidable and bad way.

I constantly have to go back to it and edit it, and that feels wrong.
Instead, I realize it can easily be generated from my blocks themselves, if I switch how I do things a bit.

Specifically, I can write the name of the function and its signature into the block, as the first line below the top line.

This means I'm always being explicit about the function argument names, or actually I could leave them out.

But I'm being explicit about the function name, but only in the block itself, not in two places.
And then changes become a lot easier.

The only problem with this is now we need a way to get around the forward reference problem again.
It shouldn't be hard to extract all the function declarations though, and put them into one place before compiling.

Maybe ctags can even do this for me?

Or we can get GPT to write a Python script for it.
Which we now have.
It's a heuristic approach, but all my function declarations are pretty simple.








Planning
----
20240618

Fixed points.

Let's say we have a prompt such as "Read the following code [...] and verify the following invariant [...]."

The LLM replies with whether or not the invariant holds.

If it does not hold, the LLM can say why, or why it is not sure.

This means that we have a new way of using the LLM to program:

We set up the invariants that must hold, and then we run the system until it converges on a solution.

One basic approach to this is as follows:

We want both the invariants and the code to be at the NL level, so we have our invariants expressed in English and our implementation as well.
The LLM is given the relevant context and asked to validate the invariant, which means it must be capable of doing so; this may require breaking down the implementation in a particular way or adding additional detail.

Here is a concrete example:

We currently have a dataloss bug, where some files aren't getting handled correctly and the user has to manually fix it.
(The data isn't permanently lost, just misplaced.)

As a human programmer, here's how I would defeat this bug:

First of all, I want to define what it is that must not be supposed to happen.
Currently the user is not sure what went wrong, but the changes aren't processed correctly and there's no error message or other explanation.
So this is what we want to define out of existence.
We want to always say that either the data will be processed fully, or there will be a message clearly shown to the user that explains what actually happened.

Now how do we do that?

In this case, we have a control flow path through certain parts of the code.
Some data is written into a file, the file is processed in several steps, and finally the operation is finished.
What we want to say is that, from the point where the operation begins, until the point where it is finished, there are only two possible outcomes.

Now we have to say what "successful processing" means and what "notifying the user" means.

These need to be clear enough that we can set a boolean to true in the code when they happen, i.e. we should be able to turn our definition into a definitive test.

In other words, it's a kind of parallel programming problem, as it's another layer over the implementation itself.

In our case, we can say that the user has to hit a key to dismiss the error message.
This gives us a concrete definition of what it means for the user to be notified.
A message has to be printed and we have to get some keyboard input.
Obviously it's still possible for this keyboard input to come from somewhere other than the user, or for the message to not be visible somehow, but we have to draw a line somewhere.
By stating it this way we make our definition of success concrete: print a message and get keyboard input acknowledging it.

The other part is probably easier: we can define the successfull processing of it as saying that at the end of the process, certain contents are in some files or in memory or whatever defines the successful processing of the data.
Again this should be something that we can make concrete enough that in any given circumstance we would be able to test it, i.e. write code that determines whether it happened or not at runtime.
(At least for some known inputs, if not all.)

But the point of this exercise is not to write tests, it's to write a proof (informally) that the code is correct.
In fact, it's not to write the proof but to get things set up such that the LLM can write it.

In our case this means looking at our code (in English, say, but it could also be the PL code) and verifying smaller invariants.

At the top level it will look like this:

When the user takes some action, the following chain of events happens: A, B, C, D.
If everything works well, then after D the following will hold.

Let's say, when the user hits "e" and edits a file, then saves and exits their editor, the result will be that the changes they made make it back into the program.
This requires a certain set of steps, for example, the data has to be in the file after the editor has exited (which we can assume), and then it has to be handled a certain way by our code (which is what we want to be sure of).

Essentially we have a "happy path" defined as a linear series of operations, one chaining into the next, with pre- and post-conditions for each, and at the end the result we care about.

Then we define the exceptions.
For each of the things that can go wrong, we need to specify how the user will be notified.

Essentially our argument will follow the branch analysis in the code.
Every time something can branch, we will generally show that either we continue on the happy path, or we go to a path that lets the user be notified.

One way to do this is to have a single top-level if statement or branch.

In other words, we have some function that returns success or failure.
We can then easily prove that either the function won't return (i.e. the program crashes) or if it does, then the program will only take one of two paths.
Then if one of those paths definitely notifies the user, that part is easy.
However, now it's required to show that if a success result is returned, it really did succeed.

Or you could have a path that either succeeds linearly and returns, or crashes, without any error return path.
Then you can prove that either it will succeed or crash, and this is another way to make sure the user gets notified.

Then we would break the argument down into steps, matching the steps of the implementation.
Then we would have prompting that checks that the implementation always always does what it needs to do for each of the steps.

The outcome of this is not actually a proof, because we don't care about that.
The outcome is a process that we can use, without human effort, to determine whether or not the code is correct.

So what we really want is an argument structure, and the mapping between this structure and the code, that the LLM can follow.
And then we want this all to be robust and tested enough that we have confidence that if something changes and breaks the LLM will likely notice it, and also that the false positives will be low enough to make the system usable.

So here's a basic sketch of how all this works in practice:

You start out with your program in English.

You then add your invariant, e.g. either feature X works with result Y or the user is notified.

Then you define the invariant well enough that it can be verified.

Then the LLM and you work together to create a proof sketch that satisfies you that the invariant is met.
Along with the sketch is information about how the parts of the proof map onto parts of the code.
For example, we can use block ids for this.

Then we may end up with some refinements of the prompts that help the LLM understand how to map the argument onto the code.
For example "the invariant holds because this function either writes data X to file Y or it exits with an error message".
That should be something that the LLM can then read, along with the code of that function, and say whether it is true or not.

To have confidence in this we should have examples of code that did not work and that the LLM can correctly identify as bad.

The idea of the whole system is that the LLM will constantly look at all the code, and check it for all the invariants we have set up.

If anything seems off, we can check it by prompting again, and if our probability is high enough that there's a real problem, then we show the human programmer the issue.

Otherwise, the system has settled, or reached a fixed point.

The whole system is a function that takes the current code and identifies any problems.
Every time you change anything, we will set it all running again.
Either the proofs will still work, or if something has changed, it will tell you what's wrong.

Other simple examples are malloc and free in C, or closing files that have been opened, or responding to HTTP requests.
In all these cases we wish to prove that whenever A happens, B also happens at some later point.
We can start by just saying "every HTTP request should get a response" but then we have to break that down until we reach the level of confidence that we need.

In other words, you only reach a fixed point when the system is reliable enough to catch real-world errors, but stable enough to be satisfied by the code you have.
So there's a certain level of detail that the system has to reach before it will settle.

Let's break down the malloc/free example for a sizeable C program.
First of all, this is a parallel problem.
We actually have specific data that malloc is used for, and for each of these, we want to know that it is not leaked.
So we can first categorize it into sub-problems by every use of malloc.

So at the top level, we have a "proof template" for mallocs, and at the top level is an assertion that every malloc has this proof template instantiated and filled out for it.
This is something an LLM can easily check, i.e. for every block in the project, either it doesn't use malloc, or if it does, then we have another block that contains a proof template that the memory is freed.

Then for the proof template it goes something like, either the memory is allocated once and used for the duration of the process, and doesn't grow further, or it grows slowly, or it grows repeatedly but it gets freed at regular intervals, or whatever it is that makes it ok.
These will be different for different use cases, and they would be proved in different ways.
Then for each one it will be proved in some suitable way.
If some memory is used in a loop to process some files, we can simply show that in every iteration of the loop the memory is freed before more memory is allocated by the loop.








"Database idea"
----
20240618

The "database idea" is a vague term for an expansion of cmpr from a single-purpose tool to more of a framework for using LLMs to manipulate code in more flexible ways.

The change starts by taking the features we already have and implementing them in terms of something more general.

I'm going to use s-exps for simplicity.

Currently the most important operations are "r" and "e".

```
(def (replace-code)
  (let* ((idx (current-block))
         (b (get-block idx))
         (nl (comment-part b))
         (prompt (comment-to-prompt nl))
         (pl (llm prompt)))
    (replace-block idx (make-block nl pl))))
```

In this implementation of "replace-code" or "r", we need to be able to read and write blocks, and break apart blocks into parts and put the parts back together.

It's assumed that the "replace-block" implementation will handle storing a rev, re-indexing, etc.

The "e" implementation might look like this:

```
(def (edit-block)
  (let* ((idx (current-block))
         (b (get-block idx))
         (tmp (make-tmp b))
         (res (run-editor tmp)))
    (if res
      (process-tmp tmp)
      (prompt "editor non-zero exit; changes abandoned"))))
```

Here we are assuming a run-editor implementation and some other functions to make this work.

A simpler example might be just our current templates themselves.

```
(def (comment-to-prompt comment)
  (concat "```c\n"
          comment
          "```\n"
          "Write the code. Reply only with code.\n"))
```

Of course the complex part here is actually the block reference expansion.

So the current iteration of cmpr is just a specialization of this more general tool.








idea
----
20240618

"English zen garden".

The idea is to recreate the CSS zen garden of yore with an English version.

The same HTML and the same essential idea of each of the original stylesheets, but expressed in English (as in my 2048).

Then because we're not trying to rip off the original actual CSS, we might try to find more abstract ways to describe the result we want, or do clean-room implementations (without looking at the CSS, just the rendering) etc.

Now we have the connection to the "database idea".

If we need to create some assets, such as background images, we can use any AI image generator.

To do that, we might want to have a prompt stored with the CSS.

This is not exactly core cmpr feature set, but it could be a shared "recipe".

In other words, we should have blocks that we can look up and just fetch, and then those blocks should do things to our blocks.

This is where the "database idea" is going.

Then the core cmpr feature set of v8 is essentially a set of keybindings to a set of blocks that implement our operations.

This means that for different tasks we could use the tool in different ways, and it becomes quite customizable.

Something like a gen-ai image feature requires network stuff, secrets (API keys) and so on.
So it's actually quite involved.
But theoretically we could have a simplified interface (a DSL) and turn our frontend into an application of that.

The idea is that then we'd have user-editable English that would generate our DSL (with some built-in or local or inexpensive LLM) and people could share these blocks, compile them locally into configuration / code, without debugging, and then interpret them.








implementation plan
----
20240618

Implementing the logprobs feature.

We get the log probs from the API by just asking for it.

Then we just have to figure out how to display them.

We can do all this with escape codes.

It's a bit like our templating or DSL or database idea again.
We could spend a little time thinking about how much of it to abstract out into general features.

In other words, we could try to make the terminal highlighting something easy enough to re-use that other people would get some other uses for it without having to do the same amount of work.








idea
----
20240618

Blocks having arguments.

The idea is to have functions that take blocks as arguments and produce blocks as output.

This is related to the idea of block references but came up while thinking about the proofs / fixed point stuff.

If we have a "proof template" this is actually a recipe for creating another block.

In general we might have blocks that run and create other blocks.

The fixed point is when all the blocks that there are are quiescent.

In other words... what if blocks could run in the background?

Then you can simply add a block to your project, and the block will run, that is, it will generate a stream of potential changes to other blocks.
If these changes are automatically accepted, then the whole system is adapting itself continuously until it settles (with more or less human involvement).








implementation plan
----
20240621

I'm going to add sqlite.

Then I'm reimplementing the undo caching.

(Update: added and tested and removed sqlite, and used the original approach instead.)








idea
----
20240621

A simple idea: comments apply to revs (a version fixed in time).
That means if you comment on a rev, then you can refer to that rev, with its comments.
For example the NL description can say "like #block@xyz" where block is a blockid and xyz is a short id string that is assigned when needed.
For example, the first time someone refers to a block, it is added to a file "blockrefs" which contains the mapping "xyz = 0123456789ABCDEF".
