cmpr design ideas / plans

Files and blocks
----

20240409 Wed

Files can be empty, but blocks are never empty, which means an empty file contains no blocks.

We maintain the clean definition of blocks (they are non-empty spans of texts) and we also respect the idea of files.

Therefore, j/k order is a union of the file list and the block list.

We do away with the earlier idea of empty blocks only in empty files.

As an example, if the project contains files a, b, c, and d containing the following blocks:

a: 1 2 3
b: no blocks
c: 4 5
d: 6

In j/k order (formally, just kidding order) we visit three blocks for file a, and the empty file b, and then the fourth block in file c.

Unlike as of v5, we do not count a block index for the empty file b, which means you can add an empty file to your project to record the intention to store some blocks there in the future without changing the number of blocks in your project.
The language of empty files does not even need to be defined, at least not for ingest.
Files and blocks are separate lists, but they have a total order.

Now we also say (as of v6, say) that every block has a checksum.
The checksum isn't guaranteed to stay the same between versions, but the tool should be responsible for updating it, which means at least one overlap between checksum implementations to support version upgrading.
If the file is very short we may just store the content itself.

For now, we are assuming that the only use we have for the checksums is that we can identify whether a file contents have changed.
If they have, we can identify changes to individual blocks if the file is divided into blocks.
However, as the checksum function we are using is of course opaque, we will not be able to measure block similarity, only content-identity, and that only probabilistically.

The concept of empty files also makes vi-like normal mode commands like d, p, o, useful to do things like take a block from an existing file with d and move it into a new file with p, even if that file was empty.
Obviously this will get more useful when we have actual content-aware hashing.








Basic operations
----
20240310

The basic operations are all on blocks.

They are:

r/R (currently split into two parts but logically one operation)
d (not implemented)
o (not implemented)
e
x (not implemented)
p/P (not implemented)

These are all modifying operations, but we also have:

undo
redo
diff

This family of operations change the block contents, but not necessarily to something new.
Undo is "u" in vim, but redo requires a modifier key.
I'm not sure if we want to keep these exact keyboard shortcuts, especially as modifier keys are difficult in some environments (e.g. browsers).

We also need new keys for the inverse operations of "r/R".
This really needs, to be full-featured, to include getting the full block into the clipboard, getting the comment part or code part, and optionally applying some transformation to either one of these.

An easy DSL to express this is the following:

prompt("writethecode",commentpart(block))

In this idea, block is a local variable, commentpart and prompt are functions, and the string literal "writethecode" is an identifier for a prompt template which in this case takes a single argument.

Arguments to prompts are always positional and can always be empty.
The prompt "writethecode" might be given on the conf file or could come from a block of prompt templates, or another conf file, e.g. a "writethecode" file in a .cmpr/prompts/ directory or similar.









Implementation plan
----
20240310

When we read in data from disk (in get_code) we should also do all indexing operations.

This means we get checksums for all our blocks and all lines.

In particular:

- we make a lines spans which we store on ui_state
- this will be kept up to date for data that is in memory only

It is intended that the backing file always contains the same lines as we have in inp.

However, as we do not exclusively own the file, we always check it when opening a project and again before writing.

Specifically, when we write, we first copy the file, if it exists, to a .bak copy.
We read the bytes of this file and compare them with what we have in memory.
Note that in new_rev we still have the previous contents of the file (since that is where we update file.contents and inp and therefore the block contents as well).




























Networking Plan
----

A revstore:

- stores blocks
- serves lists of block hashes which it has
- serves block contents for a given hash when asked
- only allows known users to write to it
- may have access controls on who can read from it
- (necessarily) has rate limits on all access for all users

Because a revstore is controlled by a certain author, it's a great place to get vetted news, if you trust that author (or curator).

Guiding principle:

Whenever possible, determine everything from the revstore contents itself.

> cmpr --rvs-server

Or perhaps just

> cmpr --server

























Outputs
----

Let's say the filetype is HTML.
It would be natural to be able to view the output (maybe continuously updated) without having to use the (B)uild command just for that.
So let's say we have "V" or :view for view, and this could include a 20px fixed header with a history bar.
The current contents will automatically refresh.

However, we might not want to treat an HTML file (particularly a generated one) as a source of blocks.
This means if we have Library: we should have View: as well.
The point is to bring the view into the workflow (for example, as an input to some procedure) but not add blocks.
Note that you can still edit a file.
The necessity for View: to open an external browser is a limitation of the terminal interface.
In particular, it will not play nicely with SSH.
A :view-url would be a nice way to get a network-friendly https url to fetch HTML from.

































Block chains
----

As a simple example of a more general idea, we propose chains of blocks as an include mechanism.

As an LLM requires input to be provided in some order, each block can strictly speaking only have one block immediately preceding it.

The idea of a block chain is just this: each block may name its immediate predecessor.

The syntax for this can be as follows:

<block-start-token> #name-of-block @predecessor-name

This suggests two basic features:

1. An index of block names to block indices.

2. A transform on blocks that expands references (transitively) and removes references if desired.

The implementation is like this:

expand_blockrefs: Block -> State BlockChain
expand_blockrefs Complete b := b
expand_blockrefs Child b := expand_blockrefs (expand_parent b)

Where expand_parent simply concatenates the parent block to the block itself.
Obviously, expand_blockrefs can be extended to have multiway branching, to intelligently handle cycles, and so on.

This can be seen as a specific case of a more general pattern:

1. look up a block
2. do something with it

In the case of r/R this is split the comment part, wrap it with a prompt template, send it to an LLM, and replace the remaining suffix of the block with the LLM output.
There's a human in the loop, which is enhanced if we have both "r" (regenerate) and "u" (undo) in the same UI.

The idea for "r" implementation is that if you turn the API on, it just does it.
Then you have "u" to go back, and perhaps some kind of diff view.
I think "U" could be for the diff view features and an undo/redo list; like search mode it would be a new view.
It might show a scrollable list of all revs of the current block.
It would show commands on the ruler, such as space to select/mark a rev, d to see diffs between marked and current, perhaps m to do a three-way merge (you select three revs, it puts all the lines that were in any of them into the same file and you edit it), and so on.

An idea nearby the block parent sigil is one for block filters, each of which is just a function applied in order.
For example:

<block_start> #some_block .func1 .func2

Would produce func2(func1(block contents)).

























Name spaces
----

A name space is a list of pairs of checksums and names.
It is published by a revstore which also hosts blocks having all the given checksums.

There can only be one name for a block in a given namespace.

As an example, I can publish at spanio.cmpr.ai my own namespace for the spanio library.

I can also publish all of the blocks.

Then you can run any of my code by getting an index block from my server, listing the name space, searching the names, and accessing any of interest.
































Third half
----

The third half is the diff layer.

If you have a block and you want a better version of that block, you want to be able to describe the difference in English and have ChatGPT make it happen.

In other words, there are two dimensions:

- block index dimension
- time dimension

Between any two blocks a, b, there is a diff d(a,b) that represents the change from a to b, a delta or derivative.

A very natural operation is describe_diff(a, b) which is like diff(a, b) but it adds an English explanation.

This can be something like "This fixes a bug in the code by changing a pointer from an incorrect to a correct value" but a more useful description would include variable names and similar detail such that the patch can likely be recreated.

In common use with ChatGPT, the user may say "the code needs to be fixed to do ..." and the LLM replies with a suggested change.
This is going from an English diff to a diff in PL code.

The reverse operation is also interesting: given a change in PL code, find the corresponding change to the English (NL) code.

In fact, as there are both NL and PL texts and also NL and PL deltas, there are at least four interesting arrows: NL to PL is handled by the LLM (mainly), PL to NL is sometimes done when debugging or when ingesting code.
A NL delta may be taken as a change to NL or to PL, similarly a PL delta may be a PL change (e.g. a standard diff) or may be interpreted (by either human or LLM) as a change to NL.

We can also define different kinds of deltas.
For example, a unified diff is a textually unambiguous delta, but a description in English of a change is also a kind of delta.
The LLM or the human may convert between these.
In particular, conversion between English descriptions of changes to the code and unambiguous diffs to the original English description is how we go from an English conversation (ending in a complete program) to a single English program (resulting in a functionally identical PL program).

For any block we can define at least three significant kinds of neighbors.
First we have the parent block in time.
This is whatever it was before you edited it into its current state.

Second we have some block chain parent.
This is the pointer to the foundational information on which we depend.
(The root node by the way could be considered implicitly to point to GPT4 as currently we don't expect it to work on other models without testing.
This is a way of saying we depend on some knowledge built into the model.)
This is a temporal relationship loosely speaking, but can be edited easily, e.g. when a bootstrap block gets split into subcomponents.

Third is nearby blocks in some metric space, such as nearby programs.
An example would be our functions that split files into blocks, which are very similar to each other but with minor differences for each PL.

























Implementation plans
----
20240310

First we set up the indices: lines, blocks, files.

Then we review this file and work out a plan.

But before any of that, it turns out, we are actually implementing openai-key support.

We get the openai-key from a file .cmpr/openai-key which must be owned and readable only by the current user (i.e. chmod 0400).

The end library experience we want should be span result = llm(span).

We want to record all our API inputs and outputs.

Before calling llm() we will call some other setup functions.

So the API we want from the rest of the code is just this:

- setup_llm(): does whatever is required to be able to use the LLM
- llm(): uses the LLM
- free_llm(): does whatever cleanup is required

All the specifics of what the LLM is and what these steps are can change by configuration.

(Basically implemented this, but with the model name instead of llm() as the function name.
 I don't think supporting just one LLM at a time is good enough since you'll likely want different ones for different blocks or different commands.
 The argument to llm(), though not settled here, turned out to be a json array of messages, each with role and content, as in the OpenAI chat API.)








Planning for block reference features
----
20240501

Currently the bootstrap prompt is special: it's generated by a script, which can pull in other blocks.
We can recreate the functionality with some more natural features:

- you have a shell block that generates output directly, e.g. `find . -type f -name '*.py'` lists files in your project.
- the output of this block can be included (indicates the idea of a block's output as an addressable blob).
- a single block can be distinguished as the bootstrap block and it will call in everything else needed.








Planning for delete
----
20240501

The plan is that when a block is deleted it will be added to a file (e.g. .cmpr/deleted) and then when it is pasted somewhere else it can come from this file.
That means we don't have to use revs for this (although we could) and we also don't have to keep deleted blocks in memory.
It also means that deleted blocks automatically persist across restarts.
A side-effect could be that, since this deleted file would be appended to rather than replaced when deleting a new block (perhaps...) then it would actually give us a stack.
So if you delete two blocks and then put two blocks, you would delete A B and then put B A.
So ddPP would reverse two blocks.
Also dp would reverse two blocks, just as ddp does in vim.
The difference is that we are assuming 'd' deletes a block, rather than 'dd' in vim for a line, since I'm assuming 'v' or other modes will be used to select multiple blocks (or a block selection mode, perhaps 's' to toggle).

However, there's a huge problem with appending multiple deleted blocks to the same 'deleted' file, namely that blocks deleted from multiple filetypes can't be stored in the same file at all.

So, instead, since we are already doing content hashing of blocks, we could have instead a deleted block directory (.cmpr/rm/ or so) and in that directory have deleted block contents, each in a file named by the blocks content hash.

Then in order to undo we need to know somehow that the block was deleted.

Or: what if we have simply a stack of deleted blocks, by content hash, and "p" by default puts the last deleted (or yanked, but we'll get to that) block into the current position, but "P" can let the user choose from all deleted blocks, most recent first.

So with undo, we have "u" to just undo the most recent change (to the current block, say, or perhaps to any block project wide as in vim), but "U" will open up a UI to choose from any number of undo dimensions (last change across project, last change or any previous version of current block, last change to current file, etc).
Then with "d" we would simply delete the current block (or selected set) while "D" would naturally give us some kind of options.
This might be: delete the file, delete the block, or maybe view deleted blocks(???)

However there are some problems with this, namely p/P is hardwired brainprint for vim folk.
I think making d immediate is fine, as it will quickly be learned, but changing something like P would just be bad.
So, perhaps a better idea is to revisit the theory of vim keybindings and come up with something similar but different.

First of all we have motion commands combined with operations, like d2j.
I think a better idea for cmpr is some basic operations like d, p, u, o, e, x, r, i, hjkl, ., and some modifiers, probably coming before them, that lead to modified operations, but not necessarily always by a motion.
For example, using ? as a placeholder for some as-yet undetermined modifier, we might have ?h giving options for moving, and ?i giving options for going into edit mode.
In fact, ? is perhaps good for this, or perhaps terrible as it's bad to type.
Many vim keybindings are specific to sub-block motion.
We have repurposed line motion to block motion, and all word or character motion does not have a direct analogue.
We can repurpose at least: a/A, b/B, c/C, e/E, f/F, H/L/M, i/I, J, K, m, q, s/S, t/T, w/W, x/X, z.
We already have plans or uses for: d, g/G, n/N, o/O, p/P, r/R, u, v, y.

Current plan: use ? as a prefix to go into an expanded "panel/help" mode with more options for a command

TODO:

bootstrap happens on startup and it tries to copy to the clipboard, which means cbcopy gets prompted








Refactoring with block references
----
20240510

Now that we have block references, it's time to split things into smaller parts and see what happens.

Here I want to have some metrics on before and after code size.

The total number of blocks currently is 172.

This can change when files change type, as some files are still "all one block" style.
So a better metric is code blocks, which we can approximate as blocks in the cmpr.c and spanio.c files.

15-116  cmpr.c
117-138 spanio.c

In spanio, since it is hand-written, there are many fewer blocks.

We can get some metrics at the command line for PL and NL lines and bytes.

Application code: 
4311 lines, 1991 NL, 2422 PL
169kB,      96kB NL, 72kB PL

Library code:
1602 lines, 417 NL, 1207 PL
52kB,      22kB NL, 30kB PL

~ $ (cd cmpr; for f in $(seq 15 116); do dist/cmpr --print-block $f; done) | wc
   4311   24640  169285
~ $ (cd cmpr; for f in $(seq 15 116); do dist/cmpr --print-comment $f; done) | wc
   1991   16903   96901
~ $ (cd cmpr; for f in $(seq 15 116); do dist/cmpr --print-code $f; done) | wc
   2422    7737   72486
~ $ (cd cmpr; for f in $(seq 117 138); do dist/cmpr --print-block $f; done) | wc
   1602    7882   52299
~ $ (cd cmpr; for f in $(seq 117 138); do dist/cmpr --print-comment $f; done) | wc
    417    3814   22369
~ $ (cd cmpr; for f in $(seq 117 138); do dist/cmpr --print-code $f; done) | wc
   1207    4068   29952

Observations:

There's more lines of PL code in cmpr than lines of NL code, but when measured in bytes, there's more NL code.
This makes sense as our lines of English prose are on average much longer (usually whole sentences).

In the library, as it's all hand-written, this doesn't tell us much.
There's a lot of commented-out code and a lot of documentation but also design rationale and so on.

As we refactor it would be interesting to see these change, so we can script this.
However, the number of blocks is going to change as we move things around, so nevermind.

We are going to probably mix some refactoring and dead code elimination, but maybe we can eliminate dead code first.
Then we can get a better sense of the change in NL code size due to block references.
I'm aware mostly of pagination-related dead code, so I'll start there.

Update: 20240513

I actually did very little refactoring as I decided instead to focus on building experience with block references by adding new features.
However, let's update the numbers above to see what I did do:

14-112  cmpr.c 
113-139 spanio.c

Before:
Application code: 
4311 lines, 1991 NL, 2422 PL
169kB,      96kB NL, 72kB PL

Library code:
1602 lines, 417 NL, 1207 PL
52kB,      22kB NL, 30kB PL

After:
Application code: 
4265 lines, 1980 NL, 2384 PL
167kB,      96kB NL, 71kB PL

Library code:
1700 lines, 504 NL, 1223 PL
57kB,      27kB NL, 30kB PL

~ $ (cd cmpr; for f in $(seq 14 112); do dist/cmpr --print-block $f; done) | wc
   4265   24287  167659
~ $ (cd cmpr; for f in $(seq 14 112); do dist/cmpr --print-comment $f; done) | wc
   1980   16740   96070
~ $ (cd cmpr; for f in $(seq 14 112); do dist/cmpr --print-code $f; done) | wc
   2384    7547   71688
~ $ (cd cmpr; for f in $(seq 113 139); do dist/cmpr --print-block $f; done) | wc
   1700    8755   57640
~ $ (cd cmpr; for f in $(seq 113 139); do dist/cmpr --print-comment $f; done) | wc
    504    4646   27313
~ $ (cd cmpr; for f in $(seq 113 139); do dist/cmpr --print-code $f; done) | wc
   1223    4109   30354

There's little change; I did clean up some pagination-related dead code.








Adding indexing and an Undo feature
----
20240513

We'll index the blocks in the current code first.
Then we'll index the blocks in the revs.

We're going to checksum every file, every block, and every line.

Since this indexing will be shared between current files and revs, we'll plan for that.

Currently we use `find_all_blocks` in `get_code` and we call `checksum_blocks` but it is currently empty.

We can store the checksums of the blocks on the state, in an array.

We can also store the checksums of all the lines.

Now we need an algorithm to identify previous versions of blocks.

Also, we might as well index the block ids now, which `get_code` should also do.

So for each block we get an id out, if any, and we get a list of 64-bit line hashes.

Now when we are doing block ID lookups, we will do it right, instead of using `find_block` which is for full-text search.

We can have on state a `block_ids` spans, having the same length as blocks itself.

Then when we need a block by ID we will just do a linear scan of `block_ids` and map that index back onto the block itself.

If we find duplicate block IDs after indexing, we can report that to the user and then continue.

We should regard the extraction of the block index itself as a function span -> span, and the indexing operation just treats this as one particular special case.
In other words, we could easily add other indexing functions in the future for other purposes.

(In fact, blocks themselves are simply an index that's treated specially in the UI.)

(We could treat ctags and similar as an index as well, and allow referencing arbitrary functions to pull their code into blocks as well.)

We also need to index when code changes, but we're doing `find_all_blocks` I think when code is being updated, so we need to do all the things in all the relevant places.

It makes sense to me if `find_all_blocks`, since it is scanning anyway, also does the full block checksum, line checksums, and block ID indexing.

Or perhaps for clarity these should be separated.

Actually, `find_all_blocks` is already doing a lot, so let's separate them.

After we find all the blocks, we will index them.
So in `get_code` we will call a new function `index_all_blocks`, which will allocate a spans and call a new `get_block_id` function, a checksum function, and a checksum lines function, for each block.

The block checksums can also go on the state in another array of u64, maybe aliased as a checksum type.

If we do these for each line, they will be big.
Well, in this codebase we have 24k lines, so still trivial, but ~3-4 orders of magnitude bigger when we count all the revs.

There is an interesting way to find similar blocks or spans using checksums of lines:

- Choose at random some set of bits (in 0-63) to use of size k (= a smaller hash).
- Set up k counters (possibly log-stochastic) of a reasonable size (easy here since blocks are small).
- For each block, for each line, increment each counter when the corresponding bit is set.
- Then to compare one block with another block, subtract the counts.

What we actually want is a Jaccard index, which is the ratio of the intersection and union of two sets.

The problem with estimating this by counting bits is that each bit will be the same 50% of the time by random chance when the lines are unrelated, which makes the counts less valuable, as each one will tend towards 50% of the counted lines.

We can solve this problem by picking some random pattern (for example 0101 or 0000 or 1111) of m bits.
Now we count the occurrence of these patterns at k positions, instead of counting k bits.
Now the probability of two different lines matching by chance is $2^m$.
The counts will be lower, and the variance will be higher.

To estimate the similarity from the counts, we take for each of the k counts, the min count for blocks A and B.
These represent probabilities of overlap between the two blocks.

If these are 0, it means the blocks are disjoint.
If the blocks are disjoint, however, it might be above 0.

We can see about that there are on average about 44 lines per block in our application code.

We might assume that between revisions there are up to 50% of lines changed.

So our problem can be stated as, for a block B of around 40 lines, and some collection of other blocks of similar size, find the ones that have 20 or lines in common, with some high probability of success of finding each one.

Here's an idea:

For the first k bits of the hash, for n lines in a set (a block) we find the most common pattern (the mode of the distribution) in the set.
For example, if k is 1, we find out whether more lines have a 1 or a 0 for the first bit of the hash.
This will be the same by chance 50% of the time for two random blocks.
However, for k=2, we are picking 1/4 of the lines at random, four different ways.
If there are repeated lines, this will clearly identify them, but let's assume that all the lines are distinct.
(We can of course make them distinct by deduplicating the lines as a first step; in particular this removes blank lines which might otherwise dominate.)

If the lines are distinct we are randomly partitioning them into 4 subsets.
Essentially we are looking for "surprising facts" about the set.

Specifically we are looking for surprising facts that are true of a surprising number of the members of the set.
This is why we want to look for high numbers, not low numbers, in the counts of the pattern.

Let's say we have 32 distinct items.
If we look at k = 2 bits there are four buckets, but in a 64-bit hash there are 63 contiguous 2-bit ranges that we can easily look at.

Let's say we do this and we find the most surprising of these facts.

In fact, let's just look at single bits.

The chance that there one particular single bit being set for all 32 distinct 64-bit hashes is approximately $2^{6}/2^{32}$.
(This is 64 times the probability of a single bit being set by chance 32 times.)
(This is actually the EV of the number of such bits, but close enough as it's closer to 0 than 1.)

Let's assume we can gain the most information the cheapest by looking for things that should happen around 1/32 times by chance in a set of 32 distinct lines, but that happen as many times as we care to look for.
For example, for 1/32 we have 5 bits, and there are 64 choose 5 = 7624512 sets of bits that we could examine (just considering bits that are set, not arbitrary patterns).
There are also heuristics we could use (like looking at single bits first, and then trying the surprising ones in combination) to find surprising facts more efficiently.

That is, we could pick 5-bit patterns, (for example, just picking five bits, with the assumption being that each of those bits must be set; then our representation of the pattern that we have found is just the set of those bits, i.e. 5 numbers in [0,63]) and examine them until we find something that is sufficiently unlikely but true about our set.
Then we can be as confident as we like, when we find another set for which this is true, about the similarity that we have found.

This seems a bit smarter than minhash to me, as minhash is finding something unlikely about a set member, but we are finding something unlikely about the set, but I don't really understand minhash.

This means, as $64 \choose 5$ is within an order of magnitude of $2^{26}$, we have a chance of finding something about as unlikely as one bit being set on every hash.
How many lines will this unlikely thing entail?

Apparently, around 11 lines.

```
import scipy.stats as stats

 # Parameters
p = 1/32
n = 32  # Adjusting n accordingly to match the lambda approximation

 # Poisson parameter
lambda_ = n * p

 # Find k such that P(X >= k) is approximately 10^-9
k = 0
cumulative_prob = 0

while True:
    cumulative_prob = stats.poisson.cdf(k, lambda_)
    if 1 - cumulative_prob <= 10**-9:
        break
    k += 1

k
```

This seems like it should objectively work pretty well.

As per ChatGPT:

To estimate the time required to evaluate 32 64-bit hashes, masking all possible patterns with five bits set (64-choose-5), we can break down the task into several steps and consider the computational cost for each step. Here's the process:

1. **Calculate the number of 64-choose-5 patterns:**
   \[
   \binom{64}{5} = \frac{64!}{5!(64-5)!}
   \]
   This number represents all possible ways to choose 5 bits out of 64.

2. **Evaluate the computational steps:**
   - For each of the 64-choose-5 patterns, you need to:
     - Apply the mask to each of the 32 hashes.
     - Check how many of these masked values have all five bits set.
     - Track the mask with the highest count of hashes having all five bits set.

3. **Estimate the number of operations:**
   - Number of masks (combinations): \( \binom{64}{5} \)
   - Number of operations per mask:
     - 32 hash evaluations (AND operation with the mask).
     - 32 bit-checks (checking if the 5 bits are set in each masked hash).

4. **Total number of operations:**
   \[
   \text{Total operations} = \binom{64}{5} \times (32 \times \text{AND} + 32 \times \text{bit-check})
   \]
   Assuming AND and bit-check operations are single CPU instructions.

5. **Performance on a modern CPU:**
   - Recent Threadripper CPUs (e.g., AMD Ryzen Threadripper 3990X) have high core counts and can execute many instructions per cycle.
   - Let's assume a conservative estimate of 4 GHz clock speed and 4 instructions per cycle.

 ### Step-by-step estimation:

1. **Calculate 64-choose-5:**
   \[
   \binom{64}{5} = \frac{64 \times 63 \times 62 \times 61 \times 60}{5 \times 4 \times 3 \times 2 \times 1} =  75287520
   \]

2. **Total operations:**
   \[
   \text{Total operations} = 75287520 \times (32 \times 2) = 75287520 \times 64 = 4818401280
   \]

3. **Instructions per second on Threadripper:**
   - Clock speed: 4 GHz = 4 \times 10^9 cycles/second
   - Instructions per cycle: 4
   - Total instructions per second: \( 4 \times 4 \times 10^9 = 16 \times 10^9 \) instructions/second

4. **Time to complete the task:**
   \[
   \text{Time (seconds)} = \frac{\text{Total operations}}{\text{Instructions per second}} = \frac{4818401280}{16 \times 10^9} \approx 0.301 \text{ seconds}
   \]

This rough estimate suggests that the task would take approximately 0.301 seconds on a modern Threadripper CPU. This assumes that the operations can be perfectly parallelized and that there are no significant overheads or bottlenecks. In practice, actual performance may vary due to factors like memory access times and CPU architecture specifics, but this provides a general idea of the computational effort involved.


One or two questionable remarks there but seems like a pretty solid analysis overall.


So here is the basic idea:

- we find a fingerprint for a target block, or for each block; this operation can be costly
- we search given a fingerprint over all other blocks; this operation needs to be cheap

In particular, we are searching backwards chronologically looking for the first block that is the most similar to the current block.

So here's what we do:

We have a function that takes an array of n-bit hashes and an integer k.
It tries all nCk masks and returns the one which matches the highest number of the hashes.
(We could double the performance by using popcnt and also finding the ones where none of the bits are set.)
(The mask is a number, this reminds me of Rabin fingerprints. Maybe it's exactly the same idea?)
Then we return the mask and the count.

We run this function on all blocks and store the results.

To find the previous version of a block, we scan the blocks most recent first.

For each block, we compare the number of lines that match that mask in the hash.
(We should also have the hashes stored for each line, of course.)

If it's 11, we probably have the same block, but even if it's 5, very likely the same with some changed lines.

The chance of 5 by random chance (with 32 lines, none actually matching) is $\approx 32 \times 1 / (2^{5})^{5}$.

So I'm going to implement this idea without any further analysis, and then focus on empirical evaluation of the results before considering alternative approaches.




HOWEVER:
----

Perhaps there's an even better idea.

After some napkin math, and the realization that at the moment, I only care about 1-by-n comparisons, i.e. find the last version of this particular block (for undo) and not n-by-n searches for all similar pairs of blocks at once, I can do something much simpler.

Namely:

For each block, we have a list of 64-bit integers, one per line.
We sort these 64-bit integers, for each block, once when we index them, and keep the sorted list (or both as a permutation).

To compare a block with a previous block, going back through the history of blocks, we simply count the number of shared lines they have in common.
This is a linear scan of ~ 40 pairs of sorted 64-bit integers, assuming ~40 lines in the average block, so about 1600 operations.

This means if we have ~100 blocks in the project and ~1000x more in the history, then even allowing for some growth in each of those numbers, we have maybe 10 million blocks total in a moderately large project.
At least until we are importing linux kernel-sized projects, a linear scan should be fine for an undo feature.
In most cases, we will only need the first result or the first few anyway, and this should be easy to find quickly.

Another point, somewhat unrelated, is that we need to know the type of a file before we can even find the blocks in it.
For revs, this will have to be heuristic since we've decided not to store that information.
So there are ways to approach this, but we also want to know what the file is, perhaps... or maybe we don't even care, just about block identity.

OK, assuming we don't care, we can identify them by a heuristic over lines that are in blocks that are in that language...

In other words, we could have another index of lines by language.
Then when we look at a file, we could look at that to identify the language to use to blockize the rev.

Or, we could even run all the blockizing routines over each file, and then see which set of blocks makes the most sense...

But there is one idea that I like in all this, which is that letting the user choose the filetype it today's conf file, and then applying that retroactively to that file's revs seems nice.

For example, if I add Markdown support, and then change a file's type to that, even though it was formerly treated as C or plain text, it makes sense to blockize the older revs as Markdown and not suddenly switch to a single-block file at some point in the history.

This suggests that we take the current files, look at each rev, and simply compare the lines in order to decide which file that is an earlier version of.
Once we make that decision, we can go backwards with it, i.e. we can treat that version n-1 as the version that we are matching against as we go even further back, and in this way we get a chain of revs for each current file.

So that's what we do, and there's now a complete implementation plan.








Implementation plan
----
20240513

For the revs and diffs and undo features.

First, we read all the files in the current project.
For each one, we hash the whole file, each line, and each block.

The lines we will store in a sorted list for the entire file.
We will do the same for the lines in each block.
We will also have a linear list of the line hashes for each file (and block).

This probably means simply that we should have a `lines` spans, like we do with blocks.
We can get the block for a line in the same way that we now get the file for a block.

A side point: separating out concepts like "here's how we handle location-based reasoning on spans" to a single block (for referencing) makes the program dramatically better for a human reader as well (previously this was implicit in the set of primitives exposed by spanio and how they were used).

We then load the revs.
For each rev we load the file, read and checksum the lines for the file, and then compare against our working set of file-versions.
Initially this working set is of course just the current projfiles.
However, when we find a new version (that is, a previous version) of a file in this set, then this replaces that file in the set.

Also, if we need to for performance reasons, we can do this only for one rev of each file, i.e. we can look at the last rev we have for each file, and then stop, temporarily, going back for that file, at least until we need to for some user action.

(We could also do all this on demand instead of on load.)

For now we assume it is fine to do the entire history in one go.

Once we have a nearest-neighbor file for a rev, we take the language that file is currently set at (from our working set) and blockify the file based on that language setting.
Then we add the blocks to another structure, which is "blocks per rev".
We checksum each block, and associate each with a (sorted and non-sorted) list of line hashes.

When the user asks to see the undo list with "U", we then go back, not just to the previous versions of this file, because that block could have been moved around, but rather to a chronological list of all blocks.
(The order of blocks from the same rev is undefined in this list, so it's a partial order.)
The first block that we find that is similar enough we display.

When we are searching through the blocks, we should, after every few loops, clear the screen, and print the progress of blocks we have searched and blocks to search through.
Once we have searched all of them, we will show the most recent matches first, and allow some kind of moving around between them with j/k.

To this screen we can eventually add other features like displaying a diff, hitting "space" to mark a particular version as the one to compare against, and so on.
But for now, we'll just show the list.
If you hit Enter on a different version, we'll just make that the current one and leave the "U" view.

So now we think about what data structures we need.

- line hashes by rev, sorted (for file identification)
- line hashes by block, sorted
- the "working set" of file revs as we go backward
  - one language per file/rev
  - the current filename, probably, if we even care(?)
  - the rev, of course (the filename)
  - the timestamp, probably converted to Unix time
- the chronological list of blocks
  - the sorted line hashes
  - the timestamp
  - the id if there was one

We should make these structures dynamic, in that they are intended to be extended (backwards through time) as they are used.
In particular, we probably will not want to populate all the block histories on startup.
However, for now, we should treat these as later performance enhancements, and start by brute-forcing everything.

We might want to have a "debug flags" conf variable.
Adding letters to this, which will be by default empty, will add debugging information in the UI, such as a cmp space high-water mark in the ruler.
Or instead of letters, maybe a comma-separated list of items like "cmp" for the aforementioned.

So now in terms of blocks and implementation:

Currently we load all the code in `get_code` and then we call `get_revs`.
This suggests we can work on everything in `get_code` first, and then revisit the implementation plan for `get_revs`.

For storing the line hashes, we will need a new structure.

We might as well have a spans for the lines, which means 64 bits * 2 per line.
This is of course wasteful, but we don't care yet.
This is 16 bytes of waste per line, and as we have seen we have about 40 bytes per line on average anyway.

We can waste the same amount again for the sorted sets per file.
We can calculate these for all revs in `get_revs`.

In `get_code`, once we have loaded the files and the blocks, we will now do all the checksumming, letting us work out the structures for the line hashes and sorted line hashes.
For example, the sorted line hash can be another copy of the same hashes in a permutation, or we could simply store the permutation (for example, as a sequence of jumps).


Note:
As seen in the following example:

`/* #print_block #print_comment #print_code #count_blocks`

Sometimes we might want a single block to have multiple ids (when it implements multiple functions).
Should we support this? Maybe?
